{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHbY-eTHAIvv",
        "outputId": "d765f8d4-82c7-47fe-8414-27293e405fb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸš€ Starting Anomaly Detection Pipeline\n",
            "\n",
            "[âš™ï¸] Processing Dataset 1/3: ../datasets/2good_reqff.csv\n",
            "     Features: ['path_length', 'body_length', 'badwords_count']\n",
            "  ðŸ“„ Loaded 287 rows from ../datasets/2good_reqff.csv\n",
            "  ðŸ§¹ Removed 0 rows with missing values\n",
            "  âœ… Final dataset: 287 rows, 3 features\n",
            "  ðŸ“ Scaling features...\n",
            "  ðŸ¤– Training autoencoder...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/divinecoder/Downloads/tmp/hackathon/myenv/lib/python3.12/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  âœ… Autoencoder training complete\n",
            "  ðŸ“Š Computing anomaly scores...\n",
            "  ðŸ“ˆ Anomaly scores - Min: 0.3580, Max: 7.0644, Mean: 0.6181\n",
            "  ðŸŒ² Training Random Forest...\n",
            "  ðŸ” Generating SHAP explanations for top 200 anomalies...\n",
            "  âœ… SHAP explanations generated\n",
            "  ðŸ’¾ All models saved to saved_models/goodbad\n",
            "  ðŸ’¾ Saved results to shap_explanations_goodbad.csv\n",
            "  ðŸ“‹ Sample Results (Top 5 Anomalies):\n",
            "method                                path body  single_q  double_q  dashes  braces  spaces  percentages  semicolons  angle_brackets  special_chars  path_length  body_length  badwords_count class  shap_path_length  shap_body_length  shap_badwords_count  shap_anomaly_score\n",
            "   GET /index.jsp?content=inside_press.htm  NaN         0         0       0       0       0            0           0               0              0         35.0          0.0             0.0  good         -0.268456         -0.016153                  0.0            0.410151\n",
            "   GET /index.jsp?content=inside_about.htm  NaN         0         0       0       0       0            0           0               0              0         35.0          0.0             0.0  good         -0.268456         -0.016153                  0.0            0.410151\n",
            "   GET /index.jsp?content=inside_about.htm  NaN         0         0       0       0       0            0           0               0              0         35.0          0.0             0.0  good         -0.268456         -0.016153                  0.0            0.410151\n",
            "   GET /index.jsp?content=inside_about.htm  NaN         0         0       0       0       0            0           0               0              0         35.0          0.0             0.0  good         -0.268456         -0.016153                  0.0            0.410151\n",
            "   GET /index.jsp?content=inside_press.htm  NaN         0         0       0       0       0            0           0               0              0         35.0          0.0             0.0  good         -0.268456         -0.016153                  0.0            0.410151\n",
            "\n",
            "  ðŸ“Š Summary Statistics:\n",
            "     - Total anomalies analyzed: 200\n",
            "     - Highest anomaly score: 7.0644\n",
            "     - Average anomaly score: 0.7195\n",
            "\n",
            "âœ… Completed processing ../datasets/2good_reqff.csv\n",
            "==================================================\n",
            "\n",
            "[âš™ï¸] Processing Dataset 2/3: ../datasets/wls_day-02.csv\n",
            "     Features: ['ProcessID', 'ParentProcessID', 'EventID']\n",
            "  ðŸ“„ Loaded 5000 rows from ../datasets/wls_day-02.csv\n",
            "  ðŸ§¹ Removed 2396 rows with missing values\n",
            "  âœ… Final dataset: 2604 rows, 3 features\n",
            "  ðŸ“ Scaling features...\n",
            "  ðŸ¤– Training autoencoder...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/divinecoder/Downloads/tmp/hackathon/myenv/lib/python3.12/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  âœ… Autoencoder training complete\n",
            "  ðŸ“Š Computing anomaly scores...\n",
            "  ðŸ“ˆ Anomaly scores - Min: 0.0539, Max: 6.6316, Mean: 0.3011\n",
            "  ðŸŒ² Training Random Forest...\n",
            "  ðŸ” Generating SHAP explanations for top 200 anomalies...\n",
            "  âœ… SHAP explanations generated\n",
            "  ðŸ’¾ All models saved to saved_models/network\n",
            "  ðŸ’¾ Saved results to shap_explanations_network.csv\n",
            "  ðŸ“‹ Sample Results (Top 5 Anomalies):\n",
            "   UserName  EventID    LogHost   LogonID DomainName ParentProcessName  ParentProcessID    ProcessName  Time  ProcessID LogonTypeDescription Source AuthenticationPackage  LogonType Destination SubjectUserName SubjectLogonID SubjectDomainName Status ServiceName FailureReason  shap_ProcessID  shap_ParentProcessID  shap_EventID  shap_anomaly_score\n",
            "Comp655648$   4688.0 Comp655648     0x3e7  Domain001          services            704.0   rundll32.exe 86400    21324.0                  NaN    NaN                   NaN        NaN         NaN             NaN            NaN               NaN    NaN         NaN           NaN       -0.608672             -0.196779           0.0            0.576615\n",
            "Comp916004$   4688.0 Comp916004     0x3e7  Domain001        Proc950869           5804.0    cscript.exe 86400    19568.0                  NaN    NaN                   NaN        NaN         NaN             NaN            NaN               NaN    NaN         NaN           NaN       -0.443401             -0.358333           0.0            0.577221\n",
            " User886483   4688.0 Comp350505 0x34d282e  Domain001        Proc412499           5196.0 Proc407594.exe 86400     1424.0                  NaN    NaN                   NaN        NaN         NaN             NaN            NaN               NaN    NaN         NaN           NaN       -0.384934             -0.414691           0.0            0.578973\n",
            "Comp916004$   4688.0 Comp916004     0x3e7  Domain001        Proc950869           5804.0    cscript.exe 86400    19648.0                  NaN    NaN                   NaN        NaN         NaN             NaN            NaN               NaN    NaN         NaN           NaN       -0.435583             -0.363031           0.0            0.579735\n",
            "Comp028683$   4688.0 Comp028683     0x3e7  Domain001          services            844.0    svchost.exe 86400    21864.0                  NaN    NaN                   NaN        NaN         NaN             NaN            NaN               NaN    NaN         NaN           NaN       -0.489617             -0.300552           0.0            0.581163\n",
            "\n",
            "  ðŸ“Š Summary Statistics:\n",
            "     - Total anomalies analyzed: 200\n",
            "     - Highest anomaly score: 6.6316\n",
            "     - Average anomaly score: 1.3544\n",
            "\n",
            "âœ… Completed processing ../datasets/wls_day-02.csv\n",
            "==================================================\n",
            "\n",
            "[âš™ï¸] Processing Dataset 3/3: ../datasets/netflow_day-02.csv\n",
            "     Features: ['Duration', 'SrcPackets', 'DstPackets', 'SrcBytes', 'DstBytes']\n",
            "  ðŸ“„ Loaded 1048575 rows from ../datasets/netflow_day-02.csv\n",
            "  ðŸ“‰ Sampled down to 5000 rows\n",
            "  ðŸ§¹ Removed 0 rows with missing values\n",
            "  âœ… Final dataset: 5000 rows, 5 features\n",
            "  ðŸ“ Scaling features...\n",
            "  ðŸ¤– Training autoencoder...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/divinecoder/Downloads/tmp/hackathon/myenv/lib/python3.12/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  âœ… Autoencoder training complete\n",
            "  ðŸ“Š Computing anomaly scores...\n",
            "  ðŸ“ˆ Anomaly scores - Min: 0.0198, Max: 31.6918, Mean: 0.1710\n",
            "  ðŸŒ² Training Random Forest...\n",
            "  ðŸ” Generating SHAP explanations for top 200 anomalies...\n",
            "  âœ… SHAP explanations generated\n",
            "  ðŸ’¾ All models saved to saved_models/host\n",
            "  ðŸ’¾ Saved results to shap_explanations_host.csv\n",
            "  ðŸ“‹ Sample Results (Top 5 Anomalies):\n",
            "  Time  Duration  SrcDevice  DstDevice  Protocol   SrcPort   DstPort  SrcPackets  DstPackets  SrcBytes  DstBytes  shap_Duration  shap_SrcPackets  shap_DstPackets  shap_SrcBytes  shap_DstBytes  shap_anomaly_score\n",
            "121173  689622.0 Comp989948 Comp730289        17       123       123      5224.0         0.0  397024.0       0.0      -0.368661        -0.918774        -0.072914      -0.077712      -0.061812            0.363811\n",
            "121053  752556.0 Comp124494 Comp253429         6 Port94511       445         0.0      6616.0       0.0  595575.0      -0.337807        -0.868480        -0.179600      -0.048423      -0.063130            0.363950\n",
            "122404  678738.0 Comp265246 Comp186884         1 Port15379 Port15379      1129.0         0.0   51934.0       0.0      -0.377774        -0.890739        -0.072914      -0.088125      -0.064897            0.364052\n",
            "121112  681280.0 Comp279702 Comp738970        17       137       137      1980.0         0.0  169776.0       0.0      -0.376939        -0.891065        -0.072903      -0.086837      -0.064872            0.367635\n",
            "121324  700319.0 Comp171178 Comp469322        17 Port73388       162      4235.0         0.0 1079359.0       0.0      -0.374496        -0.893781        -0.072914      -0.089426      -0.064872            0.368550\n",
            "\n",
            "  ðŸ“Š Summary Statistics:\n",
            "     - Total anomalies analyzed: 200\n",
            "     - Highest anomaly score: 31.6918\n",
            "     - Average anomaly score: 1.9228\n",
            "\n",
            "âœ… Completed processing ../datasets/netflow_day-02.csv\n",
            "==================================================\n",
            "\n",
            "ðŸŽ‰ Pipeline completed successfully!\n",
            "ðŸ“ All models saved in 'saved_models' directory\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import shap\n",
        "import warnings\n",
        "import joblib\n",
        "import pickle\n",
        "warnings.filterwarnings('ignore', category=UserWarning, module='tqdm')\n",
        "\n",
        "def hex_to_int(val):\n",
        "    try:\n",
        "        if isinstance(val, str) and val.startswith(\"0x\"):\n",
        "            return int(val, 16)\n",
        "        return int(val)\n",
        "    except:\n",
        "        return np.nan\n",
        "\n",
        "def load_and_preprocess(csv_path, feature_cols, max_rows=5000):\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path)\n",
        "        print(f\"  ðŸ“„ Loaded {len(df)} rows from {csv_path}\")\n",
        "        \n",
        "        if len(df) > max_rows:\n",
        "            df = df.sample(n=max_rows, random_state=42)\n",
        "            print(f\"  ðŸ“‰ Sampled down to {max_rows} rows\")\n",
        "\n",
        "        initial_rows = len(df)\n",
        "        df = df.dropna(subset=feature_cols)\n",
        "        print(f\"  ðŸ§¹ Removed {initial_rows - len(df)} rows with missing values\")\n",
        "\n",
        "        for col in feature_cols:\n",
        "            df[col] = df[col].apply(hex_to_int)\n",
        "\n",
        "        df = df.dropna(subset=feature_cols)\n",
        "        df[feature_cols] = df[feature_cols].astype(np.float32)\n",
        "        \n",
        "        print(f\"  âœ… Final dataset: {len(df)} rows, {len(feature_cols)} features\")\n",
        "        return df, df[feature_cols].values\n",
        "        \n",
        "    except FileNotFoundError:\n",
        "        print(f\"  âŒ File not found: {csv_path}\")\n",
        "        return None, None\n",
        "    except Exception as e:\n",
        "        print(f\"  âŒ Error processing {csv_path}: {str(e)}\")\n",
        "        return None, None\n",
        "\n",
        "def train_autoencoder(X_train):\n",
        "    print(\"  ðŸ¤– Training autoencoder...\")\n",
        "    model = Sequential([\n",
        "        Dense(2, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "        Dense(X_train.shape[1], activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='mae')\n",
        "    model.fit(X_train, X_train, epochs=20, batch_size=32, verbose=0)\n",
        "    print(\"  âœ… Autoencoder training complete\")\n",
        "    return model\n",
        "\n",
        "def compute_anomaly_scores(autoencoder, X):\n",
        "    print(\"  ðŸ“Š Computing anomaly scores...\")\n",
        "    recon = autoencoder.predict(X, verbose=0)\n",
        "    scores = np.mean(np.abs(X - recon), axis=1)\n",
        "    print(f\"  ðŸ“ˆ Anomaly scores - Min: {scores.min():.4f}, Max: {scores.max():.4f}, Mean: {scores.mean():.4f}\")\n",
        "    return scores\n",
        "\n",
        "def explain_with_shap(model, X, feature_names, scores, top_n=200):\n",
        "    print(f\"  ðŸ” Generating SHAP explanations for top {top_n} anomalies...\")\n",
        "    top_idx = np.argsort(scores)[-top_n:]\n",
        "    X_top = X[top_idx]\n",
        "    scores_top = scores[top_idx]\n",
        "    \n",
        "    explainer = shap.TreeExplainer(model, data=X_top, feature_perturbation=\"interventional\")\n",
        "    shap_values = explainer.shap_values(X_top, approximate=True)\n",
        "    shap_df = pd.DataFrame(shap_values, columns=feature_names)\n",
        "    shap_df[\"anomaly_score\"] = scores_top\n",
        "    \n",
        "    print(\"  âœ… SHAP explanations generated\")\n",
        "    return shap_df, top_idx, explainer\n",
        "\n",
        "def save_models(dataset_name, autoencoder, scaler, rf_model, explainer, feature_names):\n",
        "    \"\"\"Save all models for a specific dataset\"\"\"\n",
        "    model_dir = f\"saved_models/{dataset_name}\"\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "    \n",
        "    # Save autoencoder (Keras model)\n",
        "    autoencoder.save(f\"{model_dir}/autoencoder.keras\")\n",
        "    \n",
        "    # Save scaler and random forest (sklearn models)\n",
        "    joblib.dump(scaler, f\"{model_dir}/scaler.pkl\")\n",
        "    joblib.dump(rf_model, f\"{model_dir}/random_forest.pkl\")\n",
        "    \n",
        "    # Save SHAP explainer and feature names\n",
        "    with open(f\"{model_dir}/shap_explainer.pkl\", 'wb') as f:\n",
        "        pickle.dump(explainer, f)\n",
        "    \n",
        "    with open(f\"{model_dir}/feature_names.pkl\", 'wb') as f:\n",
        "        pickle.dump(feature_names, f)\n",
        "    \n",
        "    print(f\"  ðŸ’¾ All models saved to {model_dir}\")\n",
        "\n",
        "# === CONFIG ===\n",
        "csv_list = [\n",
        "    (\"../datasets/2good_reqff.csv\", [\"path_length\", \"body_length\", \"badwords_count\"], \"shap_explanations_goodbad.csv\", \"goodbad\"),\n",
        "    (\"../datasets/wls_day-02.csv\", [\"ProcessID\", \"ParentProcessID\", \"EventID\"], \"shap_explanations_network.csv\", \"network\"),\n",
        "    (\"../datasets/netflow_day-02.csv\", [\"Duration\", \"SrcPackets\", \"DstPackets\", \"SrcBytes\", \"DstBytes\"], \"shap_explanations_host.csv\", \"host\")\n",
        "]\n",
        "\n",
        "# Create main models directory\n",
        "os.makedirs(\"saved_models\", exist_ok=True)\n",
        "\n",
        "# === PIPELINE ===\n",
        "print(\"ðŸš€ Starting Anomaly Detection Pipeline\\n\")\n",
        "\n",
        "for i, (path, features, out_csv, model_name) in enumerate(csv_list, 1):\n",
        "    print(f\"[âš™ï¸] Processing Dataset {i}/3: {path}\")\n",
        "    print(f\"     Features: {features}\")\n",
        "    \n",
        "    df, X = load_and_preprocess(path, features)\n",
        "    if df is None:\n",
        "        print(\"     â­ï¸  Skipping to next dataset\\n\")\n",
        "        continue\n",
        "    \n",
        "    print(\"  ðŸ“ Scaling features...\")\n",
        "    scaler = StandardScaler().fit(X)\n",
        "    X_scaled = scaler.transform(X)\n",
        "    \n",
        "    ae = train_autoencoder(X_scaled)\n",
        "    scores = compute_anomaly_scores(ae, X_scaled)\n",
        "    \n",
        "    print(\"  ðŸŒ² Training Random Forest...\")\n",
        "    rf = RandomForestRegressor(n_estimators=100, random_state=42).fit(X_scaled, scores)\n",
        "    \n",
        "    shap_df, top_idx, explainer = explain_with_shap(rf, X_scaled, features, scores)\n",
        "    \n",
        "    # Save all models\n",
        "    save_models(model_name, ae, scaler, rf, explainer, features)\n",
        "    \n",
        "    final_df = pd.concat([\n",
        "        df.iloc[top_idx].reset_index(drop=True), \n",
        "        shap_df.add_prefix(\"shap_\")\n",
        "    ], axis=1)\n",
        "    \n",
        "    final_df.to_csv(out_csv, index=False)\n",
        "    print(f\"  ðŸ’¾ Saved results to {out_csv}\")\n",
        "    \n",
        "    print(f\"  ðŸ“‹ Sample Results (Top 5 Anomalies):\")\n",
        "    print(final_df.head().to_string(index=False))\n",
        "    print(f\"\\n  ðŸ“Š Summary Statistics:\")\n",
        "    print(f\"     - Total anomalies analyzed: {len(final_df)}\")\n",
        "    print(f\"     - Highest anomaly score: {final_df['shap_anomaly_score'].max():.4f}\")\n",
        "    print(f\"     - Average anomaly score: {final_df['shap_anomaly_score'].mean():.4f}\")\n",
        "    \n",
        "    print(f\"\\nâœ… Completed processing {path}\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "print(\"ðŸŽ‰ Pipeline completed successfully!\")\n",
        "print(\"ðŸ“ All models saved in 'saved_models' directory\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "25sSXNVxS1q2",
        "outputId": "dc303d30-008b-41a6-f049-baf0f445f12a"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'wls_day-02.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dense\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Load CSV\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwls_day-02.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# update with your path if needed\u001b[39;00m\n\u001b[32m     11\u001b[39m feature_cols = [\u001b[33m\"\u001b[39m\u001b[33mProcessID\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mParentProcessID\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mLogonID\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mEventID\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Convert hex to int\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/tmp/hackathon/myenv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/tmp/hackathon/myenv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/tmp/hackathon/myenv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/tmp/hackathon/myenv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/tmp/hackathon/myenv/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
            "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'wls_day-02.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Load CSV\n",
        "df = pd.read_csv(\"wls_day-02.csv\")  # update with your path if needed\n",
        "feature_cols = [\"ProcessID\", \"ParentProcessID\", \"LogonID\", \"EventID\"]\n",
        "\n",
        "# Convert hex to int\n",
        "def hex_to_int(val):\n",
        "    try:\n",
        "        if isinstance(val, str) and val.startswith(\"0x\"):\n",
        "            return int(val, 16)\n",
        "        return int(val)\n",
        "    except:\n",
        "        return np.nan\n",
        "\n",
        "for col in feature_cols:\n",
        "    df[col] = df[col].apply(hex_to_int)\n",
        "\n",
        "df.dropna(subset=feature_cols, inplace=True)\n",
        "\n",
        "# Scale\n",
        "scaler = StandardScaler()\n",
        "X = df[feature_cols].astype(np.float32)\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Autoencoder\n",
        "model = Sequential([\n",
        "    Dense(2, activation='relu', input_shape=(X_scaled.shape[1],)),\n",
        "    Dense(X_scaled.shape[1], activation='sigmoid')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='mae')\n",
        "history = model.fit(X_scaled, X_scaled, epochs=20, batch_size=32, verbose=0)\n",
        "\n",
        "# Reconstruction error\n",
        "recon = model.predict(X_scaled, verbose=0)\n",
        "errors = np.mean(np.abs(X_scaled - recon), axis=1)\n",
        "\n",
        "# Assume top 5% anomalies\n",
        "threshold = np.percentile(errors, 45)\n",
        "preds = (errors > threshold).astype(int)\n",
        "labels = np.zeros(len(errors))\n",
        "labels[errors > threshold] = 1\n",
        "\n",
        "# Metrics\n",
        "print(\"Precision:\", precision_score(labels, preds))\n",
        "print(\"Recall:\", recall_score(labels, preds))\n",
        "print(\"F1:\", f1_score(labels, preds))\n",
        "print(\"ROC-AUC:\", roc_auc_score(labels, errors))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(labels, preds))\n",
        "\n",
        "# Loss plot\n",
        "plt.plot(history.history['loss'])\n",
        "plt.title(\"Autoencoder Training Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"MAE Loss\")\n",
        "plt.grid()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ww6Md0lUD7-H",
        "outputId": "aebb9475-c0a5-4453-ef91-c35d689db002"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'_id': ObjectId('6894ffdd874daee6c347819d'), 'path_length': -4829.610679602709, 'body_length': -5071.62602397515, 'badwords_count': -746.8810707360627, 'anomaly_score': 0.07648983, 'explanation': 'Log flagged due to low body_length and low path_length contributing to anomaly score.'}\n",
            "{'_id': ObjectId('6894ffdd874daee6c347819e'), 'path_length': -4799.939513546723, 'body_length': -5069.499950897789, 'badwords_count': -745.4170540314017, 'anomaly_score': 33.337746, 'explanation': 'Log flagged due to low body_length and low path_length contributing to anomaly score.'}\n",
            "{'_id': ObjectId('6894ffdd874daee6c347819f'), 'path_length': -4810.467175360215, 'body_length': -5064.038138792648, 'badwords_count': -746.6811775439102, 'anomaly_score': 27.007772, 'explanation': 'Log flagged due to low body_length and low path_length contributing to anomaly score.'}\n",
            "{'_id': ObjectId('6894ffdd874daee6c34781a0'), 'path_length': -4718.863101373702, 'body_length': -5109.013793914081, 'badwords_count': -745.3171399727747, 'anomaly_score': 75.00023, 'explanation': 'Log flagged due to low body_length and low path_length contributing to anomaly score.'}\n",
            "{'_id': ObjectId('6894ffdd874daee6c34781a1'), 'path_length': -4669.151650568704, 'body_length': -5087.3096556579385, 'badwords_count': -744.7329579157633, 'anomaly_score': 147.0, 'explanation': 'Log flagged due to low body_length and low path_length contributing to anomaly score.'}\n"
          ]
        }
      ],
      "source": [
        "from pymongo import MongoClient\n",
        "\n",
        "uri = \"mongodb+srv://shiv:Shiv123456@cluster0.v6ybl9l.mongodb.net/?retryWrites=true&w=majority\"\n",
        "client = MongoClient(uri)\n",
        "\n",
        "db = client[\"log_analysis\"]\n",
        "collection = db[\"shap_explanations\"]\n",
        "\n",
        "# Print 5 documents\n",
        "for doc in collection.find().limit(5):\n",
        "    print(doc)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
